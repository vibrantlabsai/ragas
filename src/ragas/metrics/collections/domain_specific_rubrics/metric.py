"""DomainSpecificRubrics metric - Modern collections implementation."""

import typing as t

from ragas.metrics.collections.base import BaseMetric
from ragas.metrics.result import MetricResult

from .util import (
    DEFAULT_REFERENCE_FREE_RUBRICS,
    DEFAULT_WITH_REFERENCE_RUBRICS,
    RubricScoreInput,
    RubricScoreOutput,
    RubricScorePrompt,
    format_rubrics,
)

if t.TYPE_CHECKING:
    from ragas.llms.base import InstructorBaseRagasLLM


class DomainSpecificRubrics(BaseMetric):
    """
    Evaluates responses using domain-specific rubrics with customizable scoring criteria.

    This metric allows you to define custom rubrics (scoring criteria) to evaluate
    LLM responses. It supports both reference-free and reference-based evaluation,
    making it flexible for various evaluation scenarios.

    The metric works by:
    1. Taking the input, response, and optionally reference/contexts
    2. Using an LLM to evaluate the response against the rubric criteria
    3. Returning a score (1-5) with detailed feedback

    Score interpretation (default rubrics):
    - Score 1: Response is entirely incorrect or irrelevant
    - Score 2: Response has partial accuracy with major errors
    - Score 3: Response is mostly accurate but lacks detail
    - Score 4: Response is accurate with minor omissions
    - Score 5: Response is completely accurate and thorough

    Usage:
        >>> from openai import AsyncOpenAI
        >>> from ragas.llms.base import llm_factory
        >>> from ragas.metrics.collections import DomainSpecificRubrics
        >>>
        >>> client = AsyncOpenAI()
        >>> llm = llm_factory("gpt-4o-mini", client=client)
        >>>
        >>> # Reference-free evaluation
        >>> metric = DomainSpecificRubrics(llm=llm)
        >>> result = await metric.ascore(
        ...     user_input="What is the capital of France?",
        ...     response="The capital of France is Paris.",
        ... )
        >>> print(f"Score: {result.value}, Feedback: {result.reason}")
        >>>
        >>> # Reference-based evaluation
        >>> metric_with_ref = DomainSpecificRubrics(llm=llm, with_reference=True)
        >>> result = await metric_with_ref.ascore(
        ...     user_input="What is the capital of France?",
        ...     response="The capital of France is Paris.",
        ...     reference="Paris is the capital and largest city of France.",
        ... )
        >>>
        >>> # Custom rubrics
        >>> custom_rubrics = {
        ...     "score1_description": "Completely wrong",
        ...     "score2_description": "Mostly wrong with some correct elements",
        ...     "score3_description": "Partially correct",
        ...     "score4_description": "Mostly correct with minor issues",
        ...     "score5_description": "Fully correct and comprehensive",
        ... }
        >>> metric_custom = DomainSpecificRubrics(llm=llm, rubrics=custom_rubrics)

    Attributes:
        llm: Modern instructor-based LLM for evaluation
        rubrics: Dictionary mapping score descriptions (e.g., "score1_description" to criteria text)
        with_reference: Whether to use reference-based evaluation (default: False)
        name: The metric name (default: "domain_specific_rubrics")
    """

    llm: "InstructorBaseRagasLLM"

    def __init__(
        self,
        llm: "InstructorBaseRagasLLM",
        rubrics: t.Optional[t.Dict[str, str]] = None,
        with_reference: bool = False,
        name: str = "domain_specific_rubrics",
        **kwargs,
    ):
        self.llm = llm
        self.with_reference = with_reference

        if rubrics is None:
            self.rubrics = (
                DEFAULT_WITH_REFERENCE_RUBRICS
                if with_reference
                else DEFAULT_REFERENCE_FREE_RUBRICS
            )
        else:
            self.rubrics = rubrics

        rubrics_text = format_rubrics(self.rubrics)
        self.scoring_prompt = RubricScorePrompt()
        self.scoring_prompt.instruction = (
            f"{self.scoring_prompt.instruction}\n\nScoring Rubrics:\n{rubrics_text}\n"
        )

        super().__init__(name=name, allowed_values=(1.0, 5.0), **kwargs)

    async def ascore(
        self,
        user_input: t.Optional[str] = None,
        response: t.Optional[str] = None,
        retrieved_contexts: t.Optional[t.List[str]] = None,
        reference_contexts: t.Optional[t.List[str]] = None,
        reference: t.Optional[str] = None,
    ) -> MetricResult:
        """
        Score a response using the rubric criteria.

        Args:
            user_input: The question or input provided to the system
            response: The response generated by the system
            retrieved_contexts: Contexts retrieved for generating the response
            reference_contexts: Reference contexts for evaluation
            reference: The reference/ground truth answer

        Returns:
            MetricResult with score (1-5) and feedback as reason
        """
        prompt_input = RubricScoreInput(
            user_input=user_input,
            response=response,
            retrieved_contexts=retrieved_contexts,
            reference_contexts=reference_contexts,
            reference=reference,
        )

        prompt_str = self.scoring_prompt.to_string(prompt_input)
        result: RubricScoreOutput = await self.llm.agenerate(
            prompt_str, RubricScoreOutput
        )

        return MetricResult(value=float(result.score), reason=result.feedback)


class RubricsScoreWithoutReference(DomainSpecificRubrics):
    """
    Convenience class for reference-free rubric-based evaluation.

    This is equivalent to DomainSpecificRubrics(with_reference=False).
    """

    def __init__(
        self,
        llm: "InstructorBaseRagasLLM",
        rubrics: t.Optional[t.Dict[str, str]] = None,
        name: str = "rubrics_score_without_reference",
        **kwargs,
    ):
        super().__init__(
            llm=llm, rubrics=rubrics, with_reference=False, name=name, **kwargs
        )


class RubricsScoreWithReference(DomainSpecificRubrics):
    """
    Convenience class for reference-based rubric-based evaluation.

    This is equivalent to DomainSpecificRubrics(with_reference=True).
    """

    def __init__(
        self,
        llm: "InstructorBaseRagasLLM",
        rubrics: t.Optional[t.Dict[str, str]] = None,
        name: str = "rubrics_score_with_reference",
        **kwargs,
    ):
        super().__init__(
            llm=llm, rubrics=rubrics, with_reference=True, name=name, **kwargs
        )
