"""InstanceSpecificRubrics metric - Modern collections implementation."""

import typing as t

from ragas.metrics.collections.base import BaseMetric
from ragas.metrics.result import MetricResult

from .util import (
    InstanceRubricScoreInput,
    InstanceRubricScoreOutput,
    InstanceRubricScorePrompt,
)

if t.TYPE_CHECKING:
    from ragas.llms.base import InstructorBaseRagasLLM


class InstanceSpecificRubrics(BaseMetric):
    """
    Evaluates responses using instance-specific rubrics where each sample has its own criteria.

    Unlike DomainSpecificRubrics which uses the same rubric for all samples, this metric
    allows each evaluation instance to define its own scoring criteria. This is useful when:
    - Different questions require different evaluation criteria
    - You want to customize scoring based on the specific task or context
    - Evaluation criteria vary across your dataset

    The metric works by:
    1. Taking the input, response, and a rubrics dictionary for each sample
    2. Using an LLM to evaluate the response against the provided rubric
    3. Returning a score with detailed feedback

    Usage:
        >>> from openai import AsyncOpenAI
        >>> from ragas.llms.base import llm_factory
        >>> from ragas.metrics.collections import InstanceSpecificRubrics
        >>>
        >>> client = AsyncOpenAI()
        >>> llm = llm_factory("gpt-4o-mini", client=client)
        >>>
        >>> metric = InstanceSpecificRubrics(llm=llm)
        >>>
        >>> # Each sample can have different rubrics
        >>> rubrics = {
        ...     "score1_description": "The response is completely off-topic",
        ...     "score2_description": "The response is partially relevant but misses key points",
        ...     "score3_description": "The response addresses the topic but lacks depth",
        ...     "score4_description": "The response is good with minor improvements needed",
        ...     "score5_description": "The response is excellent and comprehensive",
        ... }
        >>>
        >>> result = await metric.ascore(
        ...     user_input="Explain quantum computing",
        ...     response="Quantum computing uses quantum bits...",
        ...     rubrics=rubrics,
        ... )
        >>> print(f"Score: {result.value}, Feedback: {result.reason}")

    Attributes:
        llm: Modern instructor-based LLM for evaluation
        name: The metric name (default: "instance_specific_rubrics")
    """

    llm: "InstructorBaseRagasLLM"

    def __init__(
        self,
        llm: "InstructorBaseRagasLLM",
        name: str = "instance_specific_rubrics",
        **kwargs,
    ):
        self.llm = llm
        self.scoring_prompt = InstanceRubricScorePrompt()

        super().__init__(name=name, allowed_values=(1.0, 5.0), **kwargs)

    async def ascore(
        self,
        rubrics: t.Dict[str, str],
        user_input: t.Optional[str] = None,
        response: t.Optional[str] = None,
        retrieved_contexts: t.Optional[t.List[str]] = None,
        reference_contexts: t.Optional[t.List[str]] = None,
        reference: t.Optional[str] = None,
    ) -> MetricResult:
        """
        Score a response using instance-specific rubric criteria.

        Args:
            rubrics: Dictionary mapping score descriptions (e.g., "score1_description") to criteria
            user_input: The question or input provided to the system
            response: The response generated by the system
            retrieved_contexts: Contexts retrieved for generating the response
            reference_contexts: Reference contexts for evaluation
            reference: The reference/ground truth answer

        Returns:
            MetricResult with score and feedback as reason

        Raises:
            ValueError: If rubrics is not provided
        """
        if not rubrics:
            raise ValueError(
                "rubrics must be provided for instance-specific evaluation"
            )

        prompt_input = InstanceRubricScoreInput(
            user_input=user_input,
            response=response,
            retrieved_contexts=retrieved_contexts,
            reference_contexts=reference_contexts,
            reference=reference,
            rubrics=rubrics,
        )

        prompt_str = self.scoring_prompt.to_string(prompt_input)
        result: InstanceRubricScoreOutput = await self.llm.agenerate(
            prompt_str, InstanceRubricScoreOutput
        )

        return MetricResult(value=float(result.score), reason=result.feedback)
