{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcdd4d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# AG-UI Integration\n",
    "Ragas can run experiments on agents that stream events via the [AG-UI protocol](https://docs.ag-ui.com/). This notebook shows how to build experiment datasets, configure metrics, and score AG-UI endpoints using the modern `@experiment` decorator pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0af3e1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Install dependencies: `pip install \"ragas[ag-ui]\" python-dotenv nest_asyncio`\n",
    "- Start an AG-UI compatible agent locally (Google ADK, PydanticAI, CrewAI, etc.)\n",
    "- Create an `.env` file with your evaluator LLM credentials (e.g. `OPENAI_API_KEY`, `GOOGLE_API_KEY`, etc.)\n",
    "- If you run this notebook, call `nest_asyncio.apply()` (shown below) so you can `await` coroutines in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b16d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ragas[ag-ui]\" python-dotenv nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486082d",
   "metadata": {},
   "source": [
    "## Imports and environment setup\n",
    "Load environment variables and import the classes used throughout the walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from ragas.dataset import Dataset\n",
    "from ragas.messages import HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "# Patch the existing notebook loop so we can await coroutines safely\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69bc6c",
   "metadata": {},
   "source": [
    "## Build single-turn experiment data\n",
    "Create dataset entries with `user_input` and `reference` using `Dataset.from_pandas()` when you only need to grade the final answer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cc334",
   "metadata": {},
   "outputs": [],
   "source": [
    "scientist_questions = Dataset.from_pandas(\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"user_input\": \"Who originated the theory of relativity?\",\n",
    "                \"reference\": \"Albert Einstein originated the theory of relativity.\",\n",
    "            },\n",
    "            {\n",
    "                \"user_input\": \"Who discovered penicillin and when?\",\n",
    "                \"reference\": \"Alexander Fleming discovered penicillin in 1928.\",\n",
    "            },\n",
    "        ]\n",
    "    ),\n",
    "    name=\"scientist_questions\",\n",
    "    backend=\"inmemory\",\n",
    ")\n",
    "\n",
    "scientist_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1bbb7",
   "metadata": {},
   "source": [
    "## Build multi-turn conversations\n",
    "\n",
    "For tool-usage and goal accuracy metrics, provide:\n",
    "- `reference_tool_calls`: Expected tool calls as JSON for `ToolCallF1`\n",
    "- `reference`: Expected outcome description for `AgentGoalAccuracyWithReference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_queries = Dataset.from_pandas(\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"user_input\": [HumanMessage(content=\"What's the weather in Paris?\")],\n",
    "                \"reference_tool_calls\": json.dumps(\n",
    "                    [{\"name\": \"get_weather\", \"args\": {\"location\": \"Paris\"}}]\n",
    "                ),\n",
    "                # Expected outcome - phrased to match what LLM extracts as end_state\n",
    "                \"reference\": \"The AI provided the current weather conditions for Paris.\",\n",
    "            },\n",
    "            {\n",
    "                \"user_input\": [\n",
    "                    HumanMessage(content=\"Is it raining in London right now?\")\n",
    "                ],\n",
    "                \"reference_tool_calls\": json.dumps(\n",
    "                    [{\"name\": \"get_weather\", \"args\": {\"location\": \"London\"}}]\n",
    "                ),\n",
    "                \"reference\": \"The AI provided the current weather conditions for London.\",\n",
    "            },\n",
    "        ]\n",
    "    ),\n",
    "    name=\"weather_queries\",\n",
    "    backend=\"inmemory\",\n",
    ")\n",
    "\n",
    "weather_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3da95",
   "metadata": {},
   "source": [
    "## Configure metrics and the evaluator LLM\n",
    "\n",
    "For single-turn Q&A experiments, we use:\n",
    "- `FactualCorrectness`: Compares response facts against reference\n",
    "- `AnswerRelevancy`: Measures how relevant the response is to the question\n",
    "- `DiscreteMetric`: Custom metric for conciseness\n",
    "\n",
    "For multi-turn agent experiments, we use:\n",
    "- `ToolCallF1`: Rule-based metric comparing actual vs expected tool calls\n",
    "- `AgentGoalAccuracyWithReference`: LLM-based metric evaluating whether the agent achieved the user's goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a59dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "from ragas.embeddings.base import embedding_factory\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics import DiscreteMetric\n",
    "from ragas.metrics.collections import (\n",
    "    AgentGoalAccuracyWithReference,\n",
    "    AnswerRelevancy,\n",
    "    FactualCorrectness,\n",
    "    ToolCallF1,\n",
    ")\n",
    "\n",
    "# Async client for evaluator prompts\n",
    "async_llm_client = AsyncOpenAI()\n",
    "evaluator_llm = llm_factory(\"gpt-4o-mini\", client=async_llm_client)\n",
    "\n",
    "embedding_client = AsyncOpenAI()\n",
    "evaluator_embeddings = embedding_factory(\n",
    "    \"openai\",\n",
    "    model=\"text-embedding-3-small\",\n",
    "    client=embedding_client,\n",
    "    interface=\"modern\",\n",
    ")\n",
    "\n",
    "conciseness_metric = DiscreteMetric(\n",
    "    name=\"conciseness\",\n",
    "    allowed_values=[\"verbose\", \"concise\"],\n",
    "    prompt=(\n",
    "        \"Is the response concise and efficiently conveys information?\\n\\n\"\n",
    "        \"Response: {response}\\n\\n\"\n",
    "        \"Answer with only 'verbose' or 'concise'.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Metrics for single-turn Q&A experiments\n",
    "qa_metrics = [\n",
    "    FactualCorrectness(\n",
    "        llm=evaluator_llm,\n",
    "        mode=\"f1\",\n",
    "        atomicity=\"high\",\n",
    "        coverage=\"high\",\n",
    "    ),\n",
    "    AnswerRelevancy(\n",
    "        llm=evaluator_llm,\n",
    "        embeddings=evaluator_embeddings,\n",
    "        strictness=2,\n",
    "    ),\n",
    "    conciseness_metric,\n",
    "]\n",
    "\n",
    "# Metrics for multi-turn agent experiments\n",
    "# - ToolCallF1: Rule-based metric for tool call accuracy\n",
    "# - AgentGoalAccuracyWithReference: LLM-based metric for goal achievement\n",
    "tool_metrics = [\n",
    "    ToolCallF1(),\n",
    "    AgentGoalAccuracyWithReference(llm=evaluator_llm),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65fe39",
   "metadata": {},
   "source": [
    "## Run experiments against a live AG-UI endpoint\n",
    "Set the endpoint URL exposed by your agent. The `run_ag_ui_row()` function calls your endpoint and returns enriched row data. Combine this with the `@experiment` decorator for evaluation pipelines.\n",
    "\n",
    "Toggle the flags when you are ready to run the experiments. In Jupyter/IPython you can `await` the experiment directly once `nest_asyncio.apply()` has been called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9808e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "AG_UI_ENDPOINT = \"http://localhost:8000\"  # Update to match your agent\n",
    "\n",
    "RUN_FACTUAL_EXPERIMENT = True\n",
    "RUN_TOOL_EXPERIMENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e80383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import experiment\n",
    "from ragas.integrations.ag_ui import run_ag_ui_row\n",
    "\n",
    "\n",
    "@experiment()\n",
    "async def factual_experiment(row):\n",
    "    \"\"\"Single-turn Q&A experiment with factual correctness scoring.\"\"\"\n",
    "    # Call AG-UI endpoint and get enriched row\n",
    "    enriched = await run_ag_ui_row(row, AG_UI_ENDPOINT, metadata=True)\n",
    "\n",
    "    # Score with factual correctness metric\n",
    "    fc_result = await qa_metrics[0].ascore(\n",
    "        response=enriched[\"response\"],\n",
    "        reference=row[\"reference\"],\n",
    "    )\n",
    "\n",
    "    # Score with answer relevancy metric\n",
    "    ar_result = await qa_metrics[1].ascore(\n",
    "        user_input=row[\"user_input\"],\n",
    "        response=enriched[\"response\"],\n",
    "    )\n",
    "\n",
    "    # Score with conciseness metric\n",
    "    concise_result = await conciseness_metric.ascore(\n",
    "        response=enriched[\"response\"],\n",
    "        llm=evaluator_llm,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        **enriched,\n",
    "        \"factual_correctness\": fc_result.value,\n",
    "        \"answer_relevancy\": ar_result.value,\n",
    "        \"conciseness\": concise_result.value,\n",
    "    }\n",
    "\n",
    "\n",
    "if RUN_FACTUAL_EXPERIMENT:\n",
    "    # Run the experiment against the dataset\n",
    "    factual_result = await factual_experiment.arun(\n",
    "        scientist_questions, name=\"scientist_qa_experiment\"\n",
    "    )\n",
    "    display(factual_result.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b731189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ragas.messages import ToolCall\n",
    "\n",
    "\n",
    "@experiment()\n",
    "async def tool_experiment(row):\n",
    "    \"\"\"Multi-turn experiment with tool call and goal accuracy scoring.\"\"\"\n",
    "    # Call AG-UI endpoint and get enriched row\n",
    "    enriched = await run_ag_ui_row(row, AG_UI_ENDPOINT)\n",
    "\n",
    "    # Parse reference_tool_calls from JSON string (e.g., from CSV)\n",
    "    ref_tool_calls_raw = row.get(\"reference_tool_calls\")\n",
    "    if isinstance(ref_tool_calls_raw, str):\n",
    "        ref_tool_calls = [ToolCall(**tc) for tc in json.loads(ref_tool_calls_raw)]\n",
    "    else:\n",
    "        ref_tool_calls = ref_tool_calls_raw or []\n",
    "\n",
    "    # Score with tool metrics using the modern collections API\n",
    "    f1_result = await tool_metrics[0].ascore(\n",
    "        user_input=enriched[\"messages\"],\n",
    "        reference_tool_calls=ref_tool_calls,\n",
    "    )\n",
    "    goal_result = await tool_metrics[1].ascore(\n",
    "        user_input=enriched[\"messages\"],\n",
    "        reference=row.get(\"reference\", \"\"),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        **enriched,\n",
    "        \"tool_call_f1\": f1_result.value,\n",
    "        \"agent_goal_accuracy\": goal_result.value,\n",
    "    }\n",
    "\n",
    "\n",
    "if RUN_TOOL_EXPERIMENT:\n",
    "    # Run the experiment against the dataset\n",
    "    tool_result = await tool_experiment.arun(\n",
    "        weather_queries, name=\"weather_tool_experiment\"\n",
    "    )\n",
    "    display(tool_result.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddcddaf-f229-4c35-9fff-cbe6b181222e",
   "metadata": {},
   "source": [
    "## Advanced: Lower-Level Control\n",
    "\n",
    "The `run_ag_ui_row()` function is the recommended API, but sometimes you need more control. You can use the lower-level `call_ag_ui_endpoint()` function directly.\n",
    "\n",
    "This approach lets you:\n",
    "- Customize event handling\n",
    "- Add per-row endpoint configuration  \n",
    "- Implement custom message processing\n",
    "- Add additional logging or debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lu6rc1abfdh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.ag_ui import (\n",
    "    call_ag_ui_endpoint,\n",
    "    convert_to_ragas_messages,\n",
    "    extract_response,\n",
    ")\n",
    "\n",
    "\n",
    "@experiment()\n",
    "async def custom_ag_ui_experiment(row):\n",
    "    \"\"\"\n",
    "    Custom experiment function with full control over endpoint calls.\n",
    "    \"\"\"\n",
    "    # Call the AG-UI endpoint directly (lower-level than run_ag_ui_row)\n",
    "    events = await call_ag_ui_endpoint(\n",
    "        endpoint_url=AG_UI_ENDPOINT,\n",
    "        user_input=row[\"user_input\"],\n",
    "        timeout=60.0,\n",
    "    )\n",
    "\n",
    "    # Convert AG-UI events to Ragas messages\n",
    "    messages = convert_to_ragas_messages(events, metadata=True)\n",
    "\n",
    "    # Extract response using helper (or custom logic)\n",
    "    response = extract_response(messages)\n",
    "\n",
    "    # Score with a custom metric\n",
    "    score_result = await conciseness_metric.ascore(\n",
    "        response=response,\n",
    "        llm=evaluator_llm,\n",
    "    )\n",
    "\n",
    "    # Return result with custom fields\n",
    "    return {\n",
    "        **row,\n",
    "        \"response\": response or \"[No response]\",\n",
    "        \"message_count\": len(messages),\n",
    "        \"conciseness\": score_result.value,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rka2eqwp7fc",
   "metadata": {},
   "source": [
    "Run the custom experiment against a dataset. The `@experiment` decorator provides `.arun()` for parallel execution and automatic result collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ppq6ahib2el",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CUSTOM_EXPERIMENT = True\n",
    "\n",
    "if RUN_CUSTOM_EXPERIMENT:\n",
    "    # Run the custom experiment\n",
    "    custom_result = await custom_ag_ui_experiment.arun(\n",
    "        scientist_questions, name=\"custom_ag_ui_experiment\"\n",
    "    )\n",
    "    display(custom_result.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lt2h1sor5wh",
   "metadata": {},
   "source": [
    "### API Comparison\n",
    "\n",
    "| API Level | Function | When to Use |\n",
    "|-----------|----------|-------------|\n",
    "| High-level | `run_ag_ui_row()` | Standard experiments - handles endpoint call, conversion, and extraction |\n",
    "| Low-level | `call_ag_ui_endpoint()` + `convert_to_ragas_messages()` | Custom event handling, per-row endpoint config, advanced debugging |\n",
    "\n",
    "Both approaches work with the `@experiment` decorator - choose based on how much control you need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
