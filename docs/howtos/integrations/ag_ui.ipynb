{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcdd4d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# AG-UI Integration\n",
    "Ragas can run experiments on agents that stream events via the [AG-UI protocol](https://docs.ag-ui.com/). This notebook shows how to build experiment datasets, configure metrics, and score AG-UI endpoints using the modern `@experiment` decorator pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0af3e1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Install dependencies: `pip install \"ragas[ag-ui]\" python-dotenv nest_asyncio`\n",
    "- Start an AG-UI compatible agent locally (Google ADK, PydanticAI, CrewAI, etc.)\n",
    "- Create an `.env` file with your evaluator LLM credentials (e.g. `OPENAI_API_KEY`, `GOOGLE_API_KEY`, etc.)\n",
    "- If you run this notebook, call `nest_asyncio.apply()` (shown below) so you can `await` coroutines in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b16d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ragas[ag-ui]\" python-dotenv nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486082d",
   "metadata": {},
   "source": [
    "## Imports and environment setup\n",
    "Load environment variables and import the classes used throughout the walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from ragas.dataset import Dataset\n",
    "from ragas.integrations.ag_ui import create_ag_ui_experiment\n",
    "from ragas.messages import HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "# Patch the existing notebook loop so we can await coroutines safely\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69bc6c",
   "metadata": {},
   "source": [
    "## Build single-turn experiment data\n",
    "Create dataset entries with `user_input` and `reference` using `Dataset.from_pandas()` when you only need to grade the final answer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cc334",
   "metadata": {},
   "outputs": [],
   "source": [
    "scientist_questions = Dataset.from_pandas(\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"user_input\": \"Who originated the theory of relativity?\",\n",
    "                \"reference\": \"Albert Einstein originated the theory of relativity.\",\n",
    "            },\n",
    "            {\n",
    "                \"user_input\": \"Who discovered penicillin and when?\",\n",
    "                \"reference\": \"Alexander Fleming discovered penicillin in 1928.\",\n",
    "            },\n",
    "        ]\n",
    "    ),\n",
    "    name=\"scientist_questions\",\n",
    "    backend=\"inmemory\",\n",
    ")\n",
    "\n",
    "scientist_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1bbb7",
   "metadata": {},
   "source": [
    "## Build multi-turn conversations\n",
    "\n",
    "For tool-usage and goal accuracy metrics, provide:\n",
    "- `reference_tool_calls`: Expected tool calls as JSON for `ToolCallF1`\n",
    "- `reference`: Expected outcome description for `AgentGoalAccuracyWithReference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_queries = Dataset.from_pandas(\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"user_input\": [HumanMessage(content=\"What's the weather in Paris?\")],\n",
    "                \"reference_tool_calls\": json.dumps(\n",
    "                    [{\"name\": \"get_weather\", \"args\": {\"location\": \"Paris\"}}]\n",
    "                ),\n",
    "                # Expected outcome - phrased to match what LLM extracts as end_state\n",
    "                \"reference\": \"The AI provided the current weather conditions for Paris.\",\n",
    "            },\n",
    "            {\n",
    "                \"user_input\": [\n",
    "                    HumanMessage(content=\"Is it raining in London right now?\")\n",
    "                ],\n",
    "                \"reference_tool_calls\": json.dumps(\n",
    "                    [{\"name\": \"get_weather\", \"args\": {\"location\": \"London\"}}]\n",
    "                ),\n",
    "                \"reference\": \"The AI provided the current weather conditions for London.\",\n",
    "            },\n",
    "        ]\n",
    "    ),\n",
    "    name=\"weather_queries\",\n",
    "    backend=\"inmemory\",\n",
    ")\n",
    "\n",
    "weather_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3da95",
   "metadata": {},
   "source": [
    "## Configure metrics and the evaluator LLM\n",
    "\n",
    "For single-turn Q&A experiments, we use:\n",
    "- `FactualCorrectness`: Compares response facts against reference\n",
    "- `AnswerRelevancy`: Measures how relevant the response is to the question\n",
    "- `DiscreteMetric`: Custom metric for conciseness\n",
    "\n",
    "For multi-turn agent experiments, we use:\n",
    "- `ToolCallF1`: Rule-based metric comparing actual vs expected tool calls\n",
    "- `AgentGoalAccuracyWithReference`: LLM-based metric evaluating whether the agent achieved the user's goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a59dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI, OpenAI\n",
    "\n",
    "from ragas.embeddings.base import embedding_factory\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics import AgentGoalAccuracyWithReference, DiscreteMetric, ToolCallF1\n",
    "from ragas.metrics.collections import AnswerRelevancy, FactualCorrectness\n",
    "\n",
    "# Async client for evaluator prompts\n",
    "async_llm_client = AsyncOpenAI()\n",
    "evaluator_llm = llm_factory(\"gpt-4o-mini\", client=async_llm_client)\n",
    "\n",
    "# Sync client for embeddings (AnswerRelevancy still makes blocking calls)\n",
    "embedding_client = OpenAI()\n",
    "evaluator_embeddings = embedding_factory(\n",
    "    \"openai\",\n",
    "    model=\"text-embedding-3-small\",\n",
    "    client=embedding_client,\n",
    "    interface=\"modern\",\n",
    ")\n",
    "\n",
    "conciseness_metric = DiscreteMetric(\n",
    "    name=\"conciseness\",\n",
    "    allowed_values=[\"verbose\", \"concise\"],\n",
    "    prompt=(\n",
    "        \"Is the response concise and efficiently conveys information?\\n\\n\"\n",
    "        \"Response: {response}\\n\\n\"\n",
    "        \"Answer with only 'verbose' or 'concise'.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Metrics for single-turn Q&A experiments\n",
    "qa_metrics = [\n",
    "    FactualCorrectness(\n",
    "        llm=evaluator_llm,\n",
    "        mode=\"f1\",\n",
    "        atomicity=\"high\",\n",
    "        coverage=\"high\",\n",
    "    ),\n",
    "    AnswerRelevancy(\n",
    "        llm=evaluator_llm,\n",
    "        embeddings=evaluator_embeddings,\n",
    "        strictness=2,\n",
    "    ),\n",
    "    conciseness_metric,\n",
    "]\n",
    "\n",
    "# Metrics for multi-turn agent experiments\n",
    "# - ToolCallF1: Rule-based metric for tool call accuracy\n",
    "# - AgentGoalAccuracyWithReference: LLM-based metric for goal achievement\n",
    "tool_metrics = [\n",
    "    ToolCallF1(),\n",
    "    AgentGoalAccuracyWithReference(llm=evaluator_llm),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65fe39",
   "metadata": {},
   "source": [
    "## Run experiments against a live AG-UI endpoint\n",
    "Set the endpoint URL exposed by your agent. The `create_ag_ui_experiment` factory returns an experiment function that can be run against datasets using `.arun()`.\n",
    "\n",
    "Toggle the flags when you are ready to run the experiments. In Jupyter/IPython you can `await` the experiment directly once `nest_asyncio.apply()` has been called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9808e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "AG_UI_ENDPOINT = \"http://localhost:8000\"  # Update to match your agent\n",
    "\n",
    "RUN_FACTUAL_EXPERIMENT = True\n",
    "RUN_TOOL_EXPERIMENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e80383",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_FACTUAL_EXPERIMENT:\n",
    "    # Create experiment function configured for Q&A testing\n",
    "    factual_experiment = create_ag_ui_experiment(\n",
    "        endpoint_url=AG_UI_ENDPOINT,\n",
    "        metrics=qa_metrics,\n",
    "        evaluator_llm=evaluator_llm,\n",
    "        metadata=True,\n",
    "    )\n",
    "\n",
    "    # Run the experiment against the dataset\n",
    "    factual_result = await factual_experiment.arun(\n",
    "        scientist_questions, name=\"scientist_qa_experiment\"\n",
    "    )\n",
    "    display(factual_result.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b731189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if RUN_TOOL_EXPERIMENT:\n",
    "    # Create experiment function configured for tool usage testing\n",
    "    tool_experiment = create_ag_ui_experiment(\n",
    "        endpoint_url=AG_UI_ENDPOINT,\n",
    "        metrics=tool_metrics,\n",
    "        evaluator_llm=evaluator_llm,\n",
    "    )\n",
    "\n",
    "    # Run the experiment against the dataset\n",
    "    tool_result = await tool_experiment.arun(\n",
    "        weather_queries, name=\"weather_tool_experiment\"\n",
    "    )\n",
    "    display(tool_result.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcddaf-f229-4c35-9fff-cbe6b181222e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
