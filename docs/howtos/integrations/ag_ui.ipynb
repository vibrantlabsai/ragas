{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcdd4d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# AG-UI Integration\n",
    "Ragas can evaluate agents that stream events via the [AG-UI protocol](https://docs.ag-ui.com/). This notebook shows how to build evaluation datasets, configure metrics, and score AG-UI endpoints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0af3e1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Install optional dependencies with `pip install \"ragas[ag-ui]\" python-dotenv nest_asyncio`\n",
    "- Start an AG-UI compatible agent locally (Google ADK, PydanticAI, CrewAI, etc.)\n",
    "- Create an `.env` file with your evaluator LLM credentials (e.g. `OPENAI_API_KEY`, `GOOGLE_API_KEY`, etc.)\n",
    "- If you run this notebook, call `nest_asyncio.apply()` (shown below) so you can `await` coroutines in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b16d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ragas[ag-ui]\" python-dotenv nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486082d",
   "metadata": {},
   "source": [
    "## Imports and environment setup\n",
    "Load environment variables and import the classes used throughout the walkthrough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c051059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from ragas.dataset_schema import EvaluationDataset, MultiTurnSample, SingleTurnSample\n",
    "from ragas.integrations.ag_ui import (\n",
    "    evaluate_ag_ui_agent,\n",
    ")\n",
    "from ragas.messages import HumanMessage, ToolCall\n",
    "\n",
    "load_dotenv()\n",
    "# Patch the existing notebook loop so we can await coroutines safely\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69bc6c",
   "metadata": {},
   "source": [
    "## Build single-turn evaluation data\n",
    "Create `SingleTurnSample` entries when you only need to grade the final answer text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "803cc334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'reference'], len=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scientist_questions = EvaluationDataset(\n",
    "    samples=[\n",
    "        SingleTurnSample(\n",
    "            user_input=\"Who originated the theory of relativity?\",\n",
    "            reference=\"Albert Einstein originated the theory of relativity.\",\n",
    "        ),\n",
    "        SingleTurnSample(\n",
    "            user_input=\"Who discovered penicillin and when?\",\n",
    "            reference=\"Alexander Fleming discovered penicillin in 1928.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "scientist_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1bbb7",
   "metadata": {},
   "source": [
    "## Build multi-turn conversations\n",
    "\n",
    "For tool-usage and goal accuracy metrics, use `MultiTurnSample` with:\n",
    "- `reference_tool_calls`: Expected tool calls for `ToolCallF1`\n",
    "- `reference`: Expected outcome description for `AgentGoalAccuracyWithReference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a55eb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'reference', 'reference_tool_calls'], len=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_queries = EvaluationDataset(\n",
    "    samples=[\n",
    "        MultiTurnSample(\n",
    "            user_input=[HumanMessage(content=\"What's the weather in Paris?\")],\n",
    "            reference_tool_calls=[\n",
    "                ToolCall(name=\"get_weather\", args={\"location\": \"Paris\"})\n",
    "            ],\n",
    "            # Expected outcome for AgentGoalAccuracyWithReference\n",
    "            # Use outcome-focused language that matches what the LLM extracts as end_state\n",
    "            reference=\"The user received the current weather conditions for Paris.\",\n",
    "        ),\n",
    "        MultiTurnSample(\n",
    "            user_input=[HumanMessage(content=\"Is it raining in London right now?\")],\n",
    "            reference_tool_calls=[\n",
    "                ToolCall(name=\"get_weather\", args={\"location\": \"London\"})\n",
    "            ],\n",
    "            reference=\"The user received the current weather conditions for London.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "weather_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3da95",
   "metadata": {},
   "source": [
    "## Configure metrics and the evaluator LLM\n",
    "\n",
    "For single-turn Q&A evaluation, we use:\n",
    "- `FactualCorrectness`: Compares response facts against reference\n",
    "- `AnswerRelevancy`: Measures how relevant the response is to the question\n",
    "- `DiscreteMetric`: Custom metric for conciseness\n",
    "\n",
    "For multi-turn agent evaluation, we use:\n",
    "- `ToolCallF1`: Rule-based metric comparing actual vs expected tool calls\n",
    "- `AgentGoalAccuracyWithReference`: LLM-based metric evaluating whether the agent achieved the user's goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a59dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI, OpenAI\n",
    "\n",
    "from ragas.embeddings.base import embedding_factory\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics import AgentGoalAccuracyWithReference, DiscreteMetric, ToolCallF1\n",
    "from ragas.metrics.collections import AnswerRelevancy, FactualCorrectness\n",
    "\n",
    "# Async client for evaluator prompts\n",
    "async_llm_client = AsyncOpenAI()\n",
    "evaluator_llm = llm_factory(\"gpt-4o-mini\", client=async_llm_client)\n",
    "\n",
    "# Sync client for embeddings (AnswerRelevancy still makes blocking calls)\n",
    "embedding_client = OpenAI()\n",
    "evaluator_embeddings = embedding_factory(\n",
    "    \"openai\",\n",
    "    model=\"text-embedding-3-small\",\n",
    "    client=embedding_client,\n",
    "    interface=\"modern\",\n",
    ")\n",
    "\n",
    "conciseness_metric = DiscreteMetric(\n",
    "    name=\"conciseness\",\n",
    "    allowed_values=[\"verbose\", \"concise\"],\n",
    "    prompt=(\n",
    "        \"Is the response concise and efficiently conveys information?\\n\\n\"\n",
    "        \"Response: {response}\\n\\n\"\n",
    "        \"Answer with only 'verbose' or 'concise'.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Metrics for single-turn Q&A evaluation\n",
    "qa_metrics = [\n",
    "    FactualCorrectness(\n",
    "        llm=evaluator_llm,\n",
    "        mode=\"f1\",\n",
    "        atomicity=\"high\",\n",
    "        coverage=\"high\",\n",
    "    ),\n",
    "    AnswerRelevancy(\n",
    "        llm=evaluator_llm,\n",
    "        embeddings=evaluator_embeddings,\n",
    "        strictness=2,\n",
    "    ),\n",
    "    conciseness_metric,\n",
    "]\n",
    "\n",
    "# Metrics for multi-turn agent evaluation\n",
    "# - ToolCallF1: Rule-based metric for tool call accuracy\n",
    "# - AgentGoalAccuracyWithReference: LLM-based metric for goal achievement\n",
    "tool_metrics = [\n",
    "    ToolCallF1(),\n",
    "    AgentGoalAccuracyWithReference(llm=evaluator_llm),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65fe39",
   "metadata": {},
   "source": [
    "## Evaluate a live AG-UI endpoint\n",
    "Set the endpoint URL exposed by your agent. Toggle the flags when you are ready to run the evaluations.\n",
    "In Jupyter/IPython you can `await` the helpers directly once `nest_asyncio.apply()` has been called.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9808e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "AG_UI_ENDPOINT = \"http://localhost:8000/backend_tool_rendering\"  # Update to match your agent\n",
    "\n",
    "RUN_FACTUAL_EVAL = True\n",
    "RUN_TOOL_EVAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79e80383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f6a7e8aa0e4a60b6454ad777937c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calling AG-UI Agent:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query 0 - Agent returned no tool/context messages; using placeholder.\n",
      "Query 1 - Agent returned no tool/context messages; using placeholder.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf98f2d3bd149d88b1a2c249736d2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>conciseness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who originated the theory of relativity?</td>\n",
       "      <td>[[no retrieved contexts provided by agent]]</td>\n",
       "      <td>Albert Einstein originated the theory of relat...</td>\n",
       "      <td>Albert Einstein originated the theory of relat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who discovered penicillin and when?</td>\n",
       "      <td>[[no retrieved contexts provided by agent]]</td>\n",
       "      <td>Hello, Penicillin was discovered in 1928 by Al...</td>\n",
       "      <td>Alexander Fleming discovered penicillin in 1928.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986765</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 user_input  \\\n",
       "0  Who originated the theory of relativity?   \n",
       "1       Who discovered penicillin and when?   \n",
       "\n",
       "                            retrieved_contexts  \\\n",
       "0  [[no retrieved contexts provided by agent]]   \n",
       "1  [[no retrieved contexts provided by agent]]   \n",
       "\n",
       "                                            response  \\\n",
       "0  Albert Einstein originated the theory of relat...   \n",
       "1  Hello, Penicillin was discovered in 1928 by Al...   \n",
       "\n",
       "                                           reference  factual_correctness  \\\n",
       "0  Albert Einstein originated the theory of relat...                  1.0   \n",
       "1   Alexander Fleming discovered penicillin in 1928.                  1.0   \n",
       "\n",
       "   answer_relevancy  conciseness  \n",
       "0          1.000000          1.0  \n",
       "1          0.986765          1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def evaluate_factual():\n",
    "    return await evaluate_ag_ui_agent(\n",
    "        endpoint_url=AG_UI_ENDPOINT,\n",
    "        dataset=scientist_questions,\n",
    "        metrics=qa_metrics,\n",
    "        evaluator_llm=evaluator_llm,\n",
    "        metadata=True,\n",
    "    )\n",
    "\n",
    "\n",
    "if RUN_FACTUAL_EVAL:\n",
    "    factual_result = await evaluate_factual()\n",
    "    factual_df = factual_result.to_pandas()\n",
    "    display(factual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b731189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ba8578e1c24efc9d759afdbb8113e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calling AG-UI Agent:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ToolCallResult received but no AIMessage found. Creating synthetic AIMessage.\n",
      "ToolCallResult received but no AIMessage found. Creating synthetic AIMessage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b67b8e99a1f4c518aa2b49afdc61d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference</th>\n",
       "      <th>reference_tool_calls</th>\n",
       "      <th>tool_call_f1</th>\n",
       "      <th>agent_goal_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'What's the weather in Paris?', '...</td>\n",
       "      <td>The user received the current weather conditio...</td>\n",
       "      <td>[{'name': 'get_weather', 'args': {'location': ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'content': 'Is it raining in London right no...</td>\n",
       "      <td>The user received the current weather conditio...</td>\n",
       "      <td>[{'name': 'get_weather', 'args': {'location': ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  [{'content': 'What's the weather in Paris?', '...   \n",
       "1  [{'content': 'Is it raining in London right no...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The user received the current weather conditio...   \n",
       "1  The user received the current weather conditio...   \n",
       "\n",
       "                                reference_tool_calls  tool_call_f1  \\\n",
       "0  [{'name': 'get_weather', 'args': {'location': ...           1.0   \n",
       "1  [{'name': 'get_weather', 'args': {'location': ...           1.0   \n",
       "\n",
       "   agent_goal_accuracy  \n",
       "0                  1.0  \n",
       "1                  1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def evaluate_tool_usage():\n",
    "    return await evaluate_ag_ui_agent(\n",
    "        endpoint_url=AG_UI_ENDPOINT,\n",
    "        dataset=weather_queries,\n",
    "        metrics=tool_metrics,\n",
    "        evaluator_llm=evaluator_llm,\n",
    "    )\n",
    "\n",
    "\n",
    "if RUN_TOOL_EVAL:\n",
    "    tool_result = await evaluate_tool_usage()\n",
    "    tool_df = tool_result.to_pandas()\n",
    "    display(tool_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9730129-8b5a-408e-a58e-f0d670cffcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
